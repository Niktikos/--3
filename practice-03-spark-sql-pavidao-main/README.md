[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/oCL9dIZC)
# Обробка даних за допомогою Spark SQL

## Огляд проєкту

> [!TIP]
> Можна виконати або за допомогою чистого `DataFrame` API, `Pandas on Spark` API, або за допомогою чистих `SQL` запитів. Вибір за вами.

Ви розробник у компанії **StudentBike**, яка займається орендою велосипедів/скутерів. У вас є певні місця ("станції"), де зберігаються ваші велосипеди. Якщо на станції немає вільних місць для велосипедів (хтось уже зарезервував або взяв велосипед) протягом певного періоду часу (`timeslot`), це означає, що бізнес йде чудово. Однак вам потрібно покращити обслуговування клієнтів, пропонуючи користувачам велосипеди, коли та в тому місці, де це для них найважливіше.

Ваше завдання — знайти найбільш важливі ("критичні") пари станції та періоду часу `(stationId, timeslot)`, щоб ваш бізнес знав, куди і коли доставити більше велосипедів.

Ваш результат має бути відсортований за цією *критичністю* (*criticality*) у порядку спадання.

## Набір даних

Набір даних містить:

### 1. Файл `input/register.csv`

Містить інформацію з вашої IoT системи моніторингу про кількість використаних і вільних слотів на ваших станціях оренди велосипедів. Кожен рядок відповідає одному запису про ситуацію на одній станції в певний момент часу.

Кожен рядок має такий формат:
```
stationId\ttimestamp\tusedslots\tfreeslots
```
де `timestamp` має формат `datetime`.

> [!NOTE]
> Перший рядок файлу містить заголовок.

> [!IMPORTANT]
> Деякі дані в наборі даних пошкоджено через тимчасові збої мережі та/або вашої системи моніторингу. Це означає, що деякі рядки характеризуються "використаними слотами (used slots) = 0" і "вільними слотами (free slots) = 0". **Ці рядки необхідно відфільтрувати** перед виконанням будь-яких операцій.

### 2. Файл `input/stations.csv`

Містить опис станцій.

Кожен рядок має такий формат:
```
stationId\tlongitude\tlatitude\tname
```

> [!NOTE]
> Перший рядок файлу містить заголовок.

## Детальний опис завдання

Кожна пара "день тижня – година" є "часовим інтервалом" (`timeslot`) і пов'язана з усіма показаннями моніторингу, пов'язаними з цією парою, незалежно від дати. Наприклад, часовий інтервал `Wednesday - 17` відповідає всім показанням, зробленим у середу з `17:00:00` до `17:59:59`.

Станція $S_i$ знаходиться в критичному стані, якщо кількість вільних слотів дорівнює `0` (всі велосипеди на станції заброньовані).

*Критичність* станції $S_i$ у часовому інтервалі $T_j$ визначається як:

```math
\frac{\text{кількість записів із числом вільних слотів, яке дорівнює нулю, для пари}_{\left(S_i,T_j\right)}}{\text{загальна кількість записів для пари}_{\left(S_i,T_j\right)}}
```

### Вимоги до реалізації

Необхідно:
1. Обчислити значення *критичності* для кожної пари $(S_i, T_j)$.
2. Вибирати лише пари, у яких значення *критичності* перевищує "мінімальний поріг критичності".
   * `Мінімальний поріг критичності` має бути параметром конфігурації програми.
3. Зберегти у вихідній папці вибрані записи, використовуючи файли `csv` (із заголовком). Зберегти лише такі атрибути:
   * ідентифікатор станції
   * день тижня
   * година
   * критичність
   * довгота станції
   * широта станції
4. Зберегти результати за зменшення критичності. Якщо є два або більше записів, що характеризуються однаковим значенням критичності, то додатково відсортувати по ідентифікатору станції (у порядку зростання). Якщо і станція та сама, то сортувати за днем тижня (за зростанням) і, нарешті, за годиною (за зростанням).

### Поради та підказки

Мова SQL, доступна в Spark SQL, має низку попередньо визначених функцій, одна з яких, `hour(timestamp)`, може використовуватися в запитах SQL або в перетворенні `selectExpr`, щоб вибрати `hour` з заданої позначки часу. Ще одна цікава функція, `date_format(timestamp,format)`, може бути використана для отримання іншої корисної інформації зі стовпця `timestamp`. Наприклад, у форматі `EE` можна отримати день тижня.

```python
new_df= df.selectExpr("date_format(timestamp,'EE') as weekday hour(timestamp) as hour")
```

Щоб вказати, що роздільником вхідних файлів CSV є спеціальний символ `tab`, установіть параметр роздільника на `\\t`, викликавши `.option("delimiter", "\\t")` під час читання вхідних даних.

## Встановлення Java 17 (Eclipse Temurin)

Для роботи Apache Spark необхідно встановити Java. Рекомендується використовувати Java 17 (Eclipse Temurin).

### Для Windows

1. Завантажте інсталятор Java 17 (Eclipse Temurin) з [офіційного сайту](https://adoptium.net/temurin/releases/?version=17).
2. Виберіть інсталятор `.msi` для вашої системи (x64 для 64-розрядних систем).
3. Запустіть інсталятор і дотримуйтесь інструкцій на екрані.
4. Після встановлення перевірте, чи правильно встановлено Java:

   ```powershell
   java -version
   ```

   Ви повинні побачити щось на зразок:

   ```
   openjdk version "17.x.x" 20xx-xx-xx
   OpenJDK Runtime Environment Temurin-17.x.x+x (build 17.x.x+x)
   OpenJDK 64-Bit Server VM Temurin-17.x.x+x (build 17.x.x+x, mixed mode, sharing)
   ```

### Для Linux

1. Додайте репозиторій Adoptium:
   ```bash
   # Для Debian/Ubuntu
   wget -O - https://packages.adoptium.net/artifactory/api/gpg/key/public | sudo apt-key add -
   echo "deb https://packages.adoptium.net/artifactory/deb $(awk -F= '/^VERSION_CODENAME/{print$2}' /etc/os-release) main" | sudo tee /etc/apt/sources.list.d/adoptium.list
   sudo apt update
   
   # Для Fedora/RHEL/CentOS
   sudo dnf install -y https://packages.adoptium.net/artifactory/rpm/centos/$(rpm --eval %{centos_ver})/x86_64/Packages/adoptium-release-$(rpm --eval %{centos_ver})-1.noarch.rpm
   ```

2. Встановіть Java 17:
   ```bash
   # Для Debian/Ubuntu
   sudo apt install temurin-17-jdk
   
   # Для Fedora/RHEL/CentOS
   sudo dnf install temurin-17-jdk
   ```

3. Перевірте встановлення:
   ```bash
   java -version
   ```

### Для macOS

1. Використовуйте Homebrew для встановлення:
   ```bash
   brew tap homebrew/cask-versions
   brew install --cask temurin@17
   ```

2. Або завантажте інсталятор `.pkg` з [офіційного сайту](https://adoptium.net/temurin/releases/?version=17).

3. Перевірте встановлення:
   ```bash
   java -version
   ```

4. Якщо у вас виникли проблеми з шляхами Java, виконайте:
   ```bash
   echo 'export PATH="/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home/bin:$PATH"' >> ~/.bashrc
   source ~/.bashrc
   ```

## Налаштування середовища розробки

Перед виконанням завдання потрібно налаштувати середовище розробки.

### Створення віртуального середовища Python

1. Для Windows:
    ```powershell
    python -m venv venv
    ```

2. Для Linux/macOS:
    ```bash
    python3 -m venv venv
    ```

### Активація віртуального середовища

1. Для Windows:
    ```powershell
    .\venv\Scripts\activate
    ```

2. Для Linux/macOS:
    ```bash
    source venv/bin/activate
    ```

> [!NOTE]
> Після активації віртуального середовища ви побачите префікс `(venv)` перед командним рядком.

### Встановлення необхідних пакетів з файлу `requirements.txt`

```bash
pip install -r requirements.txt
```

## Покроковий план виконання завдання

Завдання знаходяться в файлі `src/tasks.py`.

> [!IMPORTANT]  
> Перевірте файл `main.py`, щоб зрозуміти, як використовуються всі функції. Це допоможе вам з правильною реалізацією.

1. Прочитайте вміст вхідного файлу `register.csv` і збережіть його у DataFrame.

    Вхідний файл має заголовок.

    Схема даних:
    * station: integer (nullable = true)
    * timestamp: timestamp (nullable = true)
    * used_slots: integer (nullable = true)
    * free_slots: integer (nullable = true)

2. Видаліть рядки де одночасно `free_slots = 0` та `used_slots = 0`

3. Нам потрібен логічний маркер, щоб побачити, заповнена станція чи ні. Це можна зробити за допомогою UDF під назвою `full(free_slots: int)`, яка повертає:
    * 1, якщо `free_slots` дорівнює 0
    * 0, якщо `free_slots` більше 0

    > Якщо ви використовуєте Pandas on Spark API, то треба самостійно застосувати цю функцію (або переписати її)

4. Створіть DataFrame з такою схемою:
    * station: integer (nullable = true)
    * dayofweek: string (nullable = true)
    * hour: integer (nullable = true)
    * fullstatus: integer (nullable = true) - 1 = full, 0 = non-full

5. Визначте одну групу для кожної комбінації `(station, dayofweek, hour)`

6. Обчисліть "критичність" для кожної групи `(station, dayofweek, hour)`, тобто для кожної пари `(station, timeslot)`.

    Критичність дорівнює середньому `fullStatus`.

7. Виберіть лише рядки з `criticality > threshold`

> [!NOTE]
> `threshold` є деякою бізнес-вимогою, тому візьміть випадкове число від `0.1` до `0.5`, яке вам подобається. Значення за замовчуванням `0.25`.

8. Прочитайте вміст вхідного файлу `stations.csv` і збережіть його у DataFrame.

    Вхідний файл має заголовок.

    Схема даних:
    * id: integer (nullable = true)
    * longitude: double (nullable = true)
    * latitude: double (nullable = true)
    * name: string (nullable = true)

9. Об'єднайте (`JOIN`) вибрані критичні часові інтервали з таблицею станцій, щоб отримати координати станцій

10. Відсортуйте вміст DataFrame, виберіть потрібні поля і збережіть в файл.

## Критерії оцінювання

Максимальна кількість балів за завдання — **20 балів**.

Оцінювання відбувається за такими критеріями:

1. Зчитування та обробка даних (6 балів):
   * Правильне зчитування вхідних файлів — 2 бали
   * Правильне фільтрування пошкоджених даних — 2 бали
   * Правильне використання UDF `full` — 2 бали

2. Обчислення критичності (6 балів):
   * Правильне обчислення дня тижня і години — 2 бали
   * Правильне групування за комбінацією `(station, dayofweek, hour)` — 2 бали
   * Правильне обчислення значення критичності — 2 бали

3. Об'єднання та фільтрація даних (4 бали):
   * Правильна фільтрація даних за порогом критичності — 2 бали
   * Правильне об'єднання (`JOIN`) з таблицею станцій — 2 бали

4. Сортування та вивід результатів (4 бали):
   * Правильне сортування результатів за заданими критеріями — 2 бали
   * Правильний вивід результатів у CSV файл — 2 бали

> [!NOTE]
> * Часткові бали можуть бути нараховані за неповні, але працюючі рішення.
> * Відсутність помилок при виконанні та правильне слідування вказівкам є обов'язковою умовою для максимальної оцінки.
> * Код має бути чистим, належним чином відформатованим та містити коментарі для пояснення основних етапів роботи.
